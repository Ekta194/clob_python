{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekta194/clob_python/blob/main/Day_5_HTML_requests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qy5x2r3w5a3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Make the HTTP request\n",
        "url = 'https://medium.com/towards-data-science/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content with Beautiful Soup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Remove unwanted elements (script, style, comments, etc.)\n",
        "for element in soup(['script', 'style', 'comment']):\n",
        "    element.extract()\n",
        "\n",
        "# Extract the visible text\n",
        "text = soup.get_text(separator=' ')\n",
        "\n",
        "# Print the extracted text\n",
        "print(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_and_parse(url):\n",
        "  response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content with Beautiful Soup\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Remove unwanted elements (script, style, comments, etc.)\n",
        "  for element in soup(['script', 'style', 'comment']):\n",
        "      element.extract()\n",
        "\n",
        "# Extract the visible text\n",
        "  text = soup.get_text(separator=' ')\n",
        "  return text\n",
        "\n",
        "print(get_and_parse('https://medium.com/towards-data-science/how-to-build-an-elt-with-python-8f5d9d75a12e'))"
      ],
      "metadata": {
        "id": "FFt4JOsI7--0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 requests-html\n"
      ],
      "metadata": {
        "id": "zCaHPJky63_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9e696c6-4821-4d5c-d507-492c73e47f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parse (from requests-html)\n",
            "  Downloading parse-1.19.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting bs4 (from requests-html)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Collecting importlib-metadata>=1.4 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.65.0)\n",
            "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (4.9.3)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.15.0)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=7750c03a68947120b5f412c9177873e23128dc61c10b5486626dc8f08caa329e\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: pyee, parse, fake-useragent, websockets, w3lib, importlib-metadata, cssselect, pyquery, pyppeteer, bs4, requests-html\n",
            "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 importlib-metadata-6.8.0 parse-1.19.1 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 w3lib-2.1.1 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import numpy as np\n",
        "import nltk.data\n",
        "from requests_html import HTMLSession"
      ],
      "metadata": {
        "id": "mIpPEu8v8AA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_elements(url,element):\n",
        "    session = HTMLSession()\n",
        "    r = session.get(url)\n",
        "    elements = r.html.find(element)\n",
        "    return elements"
      ],
      "metadata": {
        "id": "d00LzkUZ8BFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://medium.com/towards-data-science/ai-scaling-why-keeping-up-is-essential-and-how-to-do-it-56be3c2e1e5'"
      ],
      "metadata": {
        "id": "mR5_9sTA8GSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = parse_elements(url, 'p')\n",
        "links = parse_elements(url, 'a')"
      ],
      "metadata": {
        "id": "xYmg7zid8HDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_detection(sentence):\n",
        "    positive_words = ('happy', 'sunny', 'positive', 'triumphant', 'optimistic', 'wonderful')\n",
        "    negative_words = ('sad', 'terrible', 'frightening', 'rainy', 'scary', 'shocked')\n",
        "\n",
        "    positive = any(sentence.count(i) > 0 for i in positive_words)\n",
        "    negative = any(sentence.count(i) > 0 for i in negative_words)\n",
        "\n",
        "    if positive == negative == False:\n",
        "        return \"neutral\"\n",
        "    elif positive != negative:\n",
        "        return \"positive\" if positive else \"negative\"\n",
        "    else:\n",
        "        return \"mixed\""
      ],
      "metadata": {
        "id": "bYNYeberVUUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#repurpose our sentiment analysis as topic detection\n",
        "#detect are people talking about data science, AI, or data in general?\n",
        "def topic_detection(sentence):\n",
        "    DataScience_words = ('data science', 'regression', 'predict', 'predictive', 'classify')\n",
        "    AI_words = ('AI', 'machine learning', 'ml', 'ML', 'NLP', 'LLM','deep learning','neural network','generative', 'GPT-3')\n",
        "    DataWords = ('analysis', 'algorithms','data','data structure','data-structure','big data')\n",
        "    DataScience = any(sentence.count(i) > 0 for i in DataScience_words)\n",
        "    AI = any(sentence.count(i) > 0 for i in AI_words)\n",
        "    data = any(sentence.count(i) > 0 for i in DataWords)\n",
        "    topics = []\n",
        "    if DataScience == True:\n",
        "        topics.append(\"Data Science\")\n",
        "    if AI == True:\n",
        "        topics.append(\"AI\")\n",
        "    if data == True:\n",
        "        topics.append(\"data\")\n",
        "    return topics\n"
      ],
      "metadata": {
        "id": "XGLV9IUtWeOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#requirements: print out a sentiment analysis after every paragraph prints out\n",
        "#print(paragraphs)\n",
        "for p in range(0, len(paragraphs)):\n",
        "    if len(paragraphs[p].text) > 50:\n",
        "      print((paragraphs[p].text))\n",
        "      print(f\"TALKS ABOUT: {topic_detection(paragraphs[p].text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NJZgGMVRmv3",
        "outputId": "a4d9724e-d5c2-4bc8-a0e3-c8c911a3d08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-3 came out in early 2020, and shocked the world.\n",
            "TALKS ABOUT: ['AI']\n",
            "For the first time, we had an AI system that could write text so humanlike that readers consistently couldn’t tell that it was AI-generated. But more than that, GPT-3 could also translate between languages, write code, answer questions, and much more.\n",
            "TALKS ABOUT: ['AI']\n",
            "And it did all thing thanks simply to its scale. GPT-3 was a much larger model, featuring more parameters than any transformer ever developed before. And it was trained using an appropriately massive amount of processing power, and a huge dataset which amounted to essentially all the text on the internet at the time.\n",
            "TALKS ABOUT: ['AI', 'data']\n",
            "The idea that simply by scaling a simple transformer architecture — an architecture which had by then already been around for three years! — we could achieve a significant level of general intelligence and human-like text generation capability came as a shock to just about everyone. But it had enormous consequences: by building GPT-3 and announcing it to the world in such a public fashion, OpenAI drew the world’s attention to the AI scaling phenomenon.\n",
            "TALKS ABOUT: ['AI']\n",
            "Since then, dozens of other companies have put forward their own superscaled AI models with increasingly astonishing capabilities. Google AI, DeepMind, Mircosoft, NVIDIA, Cohere, AI21Labs, and many more Western groups have entered the fray. And Chinese labs at Huawei, Inspur, the Beijing Academy of AI, and Tsinghua University have been close to the frontier of AI scaling, too — despite export controls that make it harder for them to get their hands on the processing power that’s so critical to scaling AI.\n",
            "TALKS ABOUT: ['AI']\n",
            "The one-fringe AI scaling strategy that OpenAI first doubled down on in 2020 is now becoming the de facto consensus approach by world-leading labs everywhere. And although the specific recipe OpenAI proposed with GPT-3 has since been shown to be sub-optimal (as DeepMind’s Chinchilla showed), the fact remains: we now have a reliable way of pumping in more scale — more data, more processing power, and larger models — and getting more intelligence and more capabilities out of our AI systems.\n",
            "TALKS ABOUT: ['AI', 'data']\n",
            "And there’s no end in sight: as the dizzying pace of AI progress in the last few months has shown, scaling just seems to keep paying dividends. Google’s Minerva model can solve undergraduate-level math problems. DeepMind’s Flamingo — and many other scaled multi-modal models since — are showing that scaling makes it possible for AI to process and output text, image, video, and other data shockingly well. Once the stuff of scifi fantasy, text-to-image, text-to-video, and really, text-to-anything models are now mundane everyday realities.\n",
            "TALKS ABOUT: ['AI', 'data']\n",
            "Missing out on just one of these developments can lead to critical blind spots. Here are some that we discuss in this first episode of the Gladstone AI podcast:\n",
            "TALKS ABOUT: ['AI']\n",
            "Keeping up with the current state of the art in AI is becoming a must — not just for data scientists, data analysts, MLEs, and so on, but also for entirely nontechnical people who, like the rest of us, are directly impacted by the state of cutting-edge AI in a way we’ve never been before.\n",
            "TALKS ABOUT: ['AI', 'data']\n",
            "This immediately implies that one of the most valuable skills of the coming AI era will be the ability to translate new technical developments into nontechnical language. As progress in AI accelerates, the gap between actual AI and perceived AI capabilities will get larger, and an organization’s ability to compete will become bottlenecked by the rate at which it can recognize and leverage the latest AI tooling. People who can live at the boundary between the technical world of AI scaling and the nontechnical world of business strategy are going to become some of the most prized assets in every organization.\n",
            "TALKS ABOUT: ['AI']\n",
            "In future episodes, we’ll look at these big “what now” questions in more detail, in the context of more recent advances in AI.\n",
            "TALKS ABOUT: ['AI']\n",
            "Co-founder of Gladstone AI 🤖 an AI safety company. Author of Quantum Mechanics Made Me Do It (preorder: shorturl.at/jtMN0).\n",
            "TALKS ABOUT: ['AI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_url = 'https://www.reddit.com/r/askmath/'\n",
        "hyperlinks = parse_elements(reddit_url, 'a')"
      ],
      "metadata": {
        "id": "S0apadGL8T1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the string versions of the hyperlinks\n",
        "for hyperlink in hyperlinks:\n",
        "    print(hyperlink.links)\n"
      ],
      "metadata": {
        "id": "6CLRABsC8T4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2229c37-7b65-419e-d44c-7a84bdad96ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'/'}\n",
            "{'https://www.reddit.com/login'}\n",
            "{'https://www.reddit.com/login'}\n",
            "{'https://ads.reddit.com?utm_source=web3x_consumer&utm_name=user_menu_cta'}\n",
            "{'https://play.google.com/store/apps/details?id=com.reddit.frontpage'}\n",
            "{'https://apps.apple.com/US/app/id1064216828'}\n",
            "{'/?feed=home'}\n",
            "{'/r/popular/'}\n",
            "{'https://reddit.com/t/valheim/'}\n",
            "{'https://reddit.com/t/genshin_impact/'}\n",
            "{'https://reddit.com/t/minecraft/'}\n",
            "{'https://reddit.com/t/pokimane/'}\n",
            "{'https://reddit.com/t/halo_infinite/'}\n",
            "{'https://reddit.com/t/call_of_duty_warzone/'}\n",
            "{'https://reddit.com/t/path_of_exile/'}\n",
            "{'https://reddit.com/t/hollow_knight_silksong/'}\n",
            "{'https://reddit.com/t/escape_from_tarkov/'}\n",
            "{'https://reddit.com/t/watch_dogs_legion/'}\n",
            "{'https://reddit.com/t/nfl/'}\n",
            "{'https://reddit.com/t/nba/'}\n",
            "{'https://reddit.com/t/megan_anderson/'}\n",
            "{'https://reddit.com/t/atlanta_hawks/'}\n",
            "{'https://reddit.com/t/los_angeles_lakers/'}\n",
            "{'https://reddit.com/t/boston_celtics/'}\n",
            "{'https://reddit.com/t/arsenal_fc/'}\n",
            "{'https://reddit.com/t/philadelphia_76ers/'}\n",
            "{'https://reddit.com/t/premier_league/'}\n",
            "{'https://reddit.com/t/ufc/'}\n",
            "{'https://reddit.com/t/gamestop/'}\n",
            "{'https://reddit.com/t/moderna/'}\n",
            "{'https://reddit.com/t/pfizer/'}\n",
            "{'https://reddit.com/t/johnson_johnson/'}\n",
            "{'https://reddit.com/t/astrazeneca/'}\n",
            "{'https://reddit.com/t/walgreens/'}\n",
            "{'https://reddit.com/t/best_buy/'}\n",
            "{'https://reddit.com/t/novavax/'}\n",
            "{'https://reddit.com/t/spacex/'}\n",
            "{'https://reddit.com/t/tesla/'}\n",
            "{'https://reddit.com/t/cardano/'}\n",
            "{'https://reddit.com/t/dogecoin/'}\n",
            "{'https://reddit.com/t/algorand/'}\n",
            "{'https://reddit.com/t/bitcoin/'}\n",
            "{'https://reddit.com/t/litecoin/'}\n",
            "{'https://reddit.com/t/basic_attention_token/'}\n",
            "{'https://reddit.com/t/bitcoin_cash/'}\n",
            "{'https://reddit.com/t/the_real_housewives_of_atlanta/'}\n",
            "{'https://reddit.com/t/the_bachelor/'}\n",
            "{'https://reddit.com/t/sister_wives/'}\n",
            "{'https://reddit.com/t/90_day_fiance/'}\n",
            "{'https://reddit.com/t/wife_swap/'}\n",
            "{'https://reddit.com/t/the_amazing_race_australia/'}\n",
            "{'https://reddit.com/t/married_at_first_sight/'}\n",
            "{'https://reddit.com/t/the_real_housewives_of_dallas/'}\n",
            "{'https://reddit.com/t/my_600lb_life/'}\n",
            "{'https://reddit.com/t/last_week_tonight_with_john_oliver/'}\n",
            "{'https://reddit.com/t/kim_kardashian/'}\n",
            "{'https://reddit.com/t/doja_cat/'}\n",
            "{'https://reddit.com/t/iggy_azalea/'}\n",
            "{'https://reddit.com/t/anya_taylorjoy/'}\n",
            "{'https://reddit.com/t/jamie_lee_curtis/'}\n",
            "{'https://reddit.com/t/natalie_portman/'}\n",
            "{'https://reddit.com/t/henry_cavill/'}\n",
            "{'https://reddit.com/t/millie_bobby_brown/'}\n",
            "{'https://reddit.com/t/tom_hiddleston/'}\n",
            "{'https://reddit.com/t/keanu_reeves/'}\n",
            "{'https://reddit.com/t/animals_and_pets/'}\n",
            "{'https://reddit.com/t/anime/'}\n",
            "{'https://reddit.com/t/art/'}\n",
            "{'https://reddit.com/t/cars_and_motor_vehicles/'}\n",
            "{'https://reddit.com/t/crafts_and_diy/'}\n",
            "{'https://reddit.com/t/culture_race_and_ethnicity/'}\n",
            "{'https://reddit.com/t/ethics_and_philosophy/'}\n",
            "{'https://reddit.com/t/fashion/'}\n",
            "{'https://reddit.com/t/food_and_drink/'}\n",
            "{'https://reddit.com/t/history/'}\n",
            "{'https://reddit.com/t/hobby/'}\n",
            "{'https://reddit.com/t/law/'}\n",
            "{'https://reddit.com/t/learning_and_education/'}\n",
            "{'https://reddit.com/t/military/'}\n",
            "{'https://reddit.com/t/movie/'}\n",
            "{'https://reddit.com/t/music/'}\n",
            "{'https://reddit.com/t/place/'}\n",
            "{'https://reddit.com/t/podcasts_and_streamers/'}\n",
            "{'https://reddit.com/t/politics/'}\n",
            "{'https://reddit.com/t/programming/'}\n",
            "{'https://reddit.com/t/reading_writing_and_literature/'}\n",
            "{'https://reddit.com/t/religion_and_spirituality/'}\n",
            "{'https://reddit.com/t/science/'}\n",
            "{'https://reddit.com/t/tabletop_game/'}\n",
            "{'https://reddit.com/t/technology/'}\n",
            "{'https://reddit.com/t/travel/'}\n",
            "{'https://www.redditinc.com'}\n",
            "{'https://ads.reddit.com?utm_source=web3x_consumer&utm_name=left_nav_cta'}\n",
            "{'https://www.reddithelp.com'}\n",
            "{'https://redditblog.com/'}\n",
            "{'https://www.redditinc.com/careers'}\n",
            "{'https://www.redditinc.com/press'}\n",
            "{'https://www.reddit.com/coins'}\n",
            "{'https://www.reddit.com/premium'}\n",
            "{'https://www.reddit.com/subreddits/a-1/'}\n",
            "{'https://www.reddit.com/posts/2022'}\n",
            "{'https://www.reddit.com/topics/a-1/'}\n",
            "{'https://www.redditinc.com/policies/content-policy'}\n",
            "{'https://www.reddit.com/policies/privacy-policy'}\n",
            "{'https://www.redditinc.com/policies/user-agreement'}\n",
            "{'https://redditinc.com'}\n",
            "{'/r/askmath/hot/'}\n",
            "{'/r/askmath/new/'}\n",
            "{'/r/askmath/top/'}\n",
            "{'/r/askmath/rising/'}\n",
            "{'?feedViewType=cardView'}\n",
            "{'?feedViewType=classicView'}\n",
            "{'/r/askmath'}\n",
            "{'/r/askmath/about/'}\n",
            "{'/r/askmath/hot/'}\n",
            "{'/r/askmath/new/'}\n",
            "{'/r/askmath/top/'}\n",
            "{'/r/askmath/rising/'}\n",
            "{'?feedViewType=cardView'}\n",
            "{'?feedViewType=classicView'}\n",
            "{'/r/askmath/comments/14urmej/raskmath_weekly_chat_thread/'}\n",
            "{'/user/AutoModerator/'}\n",
            "{'/r/askmath/comments/145lbfv/raskmath_will_be_going_dark_on_june_12_in_protest/'}\n",
            "{'/user/Megame50/'}\n",
            "{'/r/ModCoord/'}\n",
            "{'/r/ModCoord/comments/13xh1e7/an_open_letter_on_the_state_of_affairs_regarding/'}\n",
            "{'/r/askmath/comments/14ws1lg/can_you_explain_why_in_simple_terms/'}\n",
            "{'/user/Gangstaspessmen/'}\n",
            "{'/r/askmath/comments/14ws1lg/can_you_explain_why_in_simple_terms/'}\n",
            "{'/user/eFax_USA/'}\n",
            "{'/r/askmath/comments/14wu4sx/help_me_with_middle_school_geometry/'}\n",
            "{'/user/Which_Ad4017/'}\n",
            "{'/r/askmath/comments/14wu4sx/help_me_with_middle_school_geometry/'}\n",
            "{'/r/askmath/comments/14wu4sx/help_me_with_middle_school_geometry/'}\n",
            "{'/r/askmath/comments/14wxffa/what_does_it_mean_to_integrate_a_variable_out/'}\n",
            "{'/user/sweett96/'}\n",
            "{'/r/askmath/comments/14wpjnz/can_someone_explain_the_pattern_used_in_the_w_list/'}\n",
            "{'/user/SimplyInept/'}\n",
            "{'https://preview.redd.it/can-someone-explain-the-pattern-used-in-the-w-list-v0-a3xj7o2otbbb1.png?width=1046&format=png&auto=webp&s=57c6c498f3ecf4d239dfaf9c8e83e05cfdd114a1'}\n",
            "{'/r/askmath/comments/14wpjnz/can_someone_explain_the_pattern_used_in_the_w_list/'}\n",
            "{'/r/askmath/comments/14wu06f/proof_for_length_of_a_helix/'}\n",
            "{'/user/Cricket2403/'}\n",
            "{'/r/askmath/comments/14wu06f/proof_for_length_of_a_helix/'}\n",
            "{'/r/askmath/comments/14wtt0e/how_to_tackle_this_dilution_problem/'}\n",
            "{'/user/sagen010/'}\n",
            "{'/r/askmath/comments/14wtt0e/how_to_tackle_this_dilution_problem/'}\n",
            "{'/r/askmath/comments/14wzrcd/need_help_for_assignment_due_today/'}\n",
            "{'/user/Ok_Cow562/'}\n",
            "{'/r/askmath/comments/14wzrcd/need_help_for_assignment_due_today/'}\n",
            "{'/r/askmath/comments/14wz8oq/direction_notation_question/'}\n",
            "{'/user/itsScylic/'}\n",
            "{'/r/askmath/comments/14wz8oq/direction_notation_question/'}\n",
            "{'/user/deepgramai/'}\n",
            "{'/r/askmath/comments/14wssbq/finding_the_root_of_a_polinimial_function_bigger/'}\n",
            "{'/user/DitadorC2/'}\n",
            "{'/r/askmath/comments/14wssbq/finding_the_root_of_a_polinimial_function_bigger/'}\n",
            "{'/r/askmath/comments/14wypqf/are_there_real_uses_for_arcsec_arccsc_arccot/'}\n",
            "{'/user/PM_TITS_GROUP/'}\n",
            "{'/r/askmath/comments/14wypqf/are_there_real_uses_for_arcsec_arccsc_arccot/'}\n",
            "{'/r/askmath/comments/14wyn98/approximating_a_periodic_function_with_a_set_of/'}\n",
            "{'/user/halfancd/'}\n",
            "{'https://www.desmos.com/calculator/k0gs12gw4e'}\n",
            "{'/r/askmath/comments/14wyn98/approximating_a_periodic_function_with_a_set_of/'}\n",
            "{'/r/askmath/comments/14wyh6k/any_better_way/'}\n",
            "{'/user/nokia_the_kokia/'}\n",
            "{'/r/askmath/comments/14wyh6k/any_better_way/'}\n",
            "{'/r/askmath/comments/14wxa97/height_of_cable_between_two_poles/'}\n",
            "{'/user/ElectricTaser/'}\n",
            "{'/r/askmath/comments/14wxa97/height_of_cable_between_two_poles/'}\n",
            "{'/r/askmath/comments/14wwzx7/finding_the_root_of_the_equation/'}\n",
            "{'/user/DitadorC2/'}\n",
            "{'https://preview.redd.it/finding-the-root-of-the-equation-v0-q7x1yr13adbb1.png?width=184&format=png&auto=webp&s=3e878cb8f6cec64be5bc6e143958cf88593c2a42'}\n",
            "{'/r/askmath/comments/14wwzx7/finding_the_root_of_the_equation/'}\n",
            "{'/r/askmath/comments/14wpq87/moment_of_inertia_of_a_solid_sphere_why_is_my/'}\n",
            "{'/user/Inside-Square1601/'}\n",
            "{'/r/askmath/comments/14wpq87/moment_of_inertia_of_a_solid_sphere_why_is_my/'}\n",
            "{'/user/psy-international/'}\n",
            "{'/r/askmath/comments/14wuxni/need_help_figuring_out_a_problem/'}\n",
            "{'/user/redwolf4221/'}\n",
            "{'/r/askmath/comments/14wuxni/need_help_figuring_out_a_problem/'}\n",
            "{'/r/askmath/comments/14wuowo/how_do_you_prove_that_a_completion_of_a_metric/'}\n",
            "{'/user/1bra/'}\n",
            "{'http://sites.iiserpune.ac.in/~supriya/teaching/Topology-MTH322/files/Completion.pdf'}\n",
            "{'/r/askmath/comments/14wuowo/how_do_you_prove_that_a_completion_of_a_metric/'}\n",
            "{'/r/askmath/comments/14wsims/can_anyone_explain_me_with_an_example_what_does_a/'}\n",
            "{'/user/sweett96/'}\n",
            "{'/r/askmath/comments/14wsims/can_anyone_explain_me_with_an_example_what_does_a/'}\n",
            "{'/r/askmath/comments/14wj7tn/derivativessimplifying_question/'}\n",
            "{'/user/Rickyyy1251/'}\n",
            "{'/r/askmath/comments/14wj7tn/derivativessimplifying_question/'}\n",
            "{'/r/askmath/comments/14wsca8/mathcombinatorics/'}\n",
            "{'/user/helpmepassmymathexam/'}\n",
            "{'/r/askmath/comments/14wsca8/mathcombinatorics/'}\n",
            "{'/r/askmath/comments/14wsb5y/normal_distribution_word_problem/'}\n",
            "{'/user/lrocky4/'}\n",
            "{'/r/askmath/comments/14wsb5y/normal_distribution_word_problem/'}\n",
            "{'/r/askmath/comments/14ws82h/card_game_odds_problem/'}\n",
            "{'/user/mehitabel_4724/'}\n",
            "{'/r/askmath/comments/14ws82h/card_game_odds_problem/'}\n",
            "{'/user/dungeonsurvivors/'}\n",
            "{'/r/askmath/comments/14wqxz5/i_need_help_with_this_doubt_its_been_bugging_me/'}\n",
            "{'/user/Comfortable_Fox6330/'}\n",
            "{'/r/askmath/comments/14wqxz5/i_need_help_with_this_doubt_its_been_bugging_me/'}\n",
            "{'/best/communities/14/#t5_2qm4f'}\n",
            "{'http://www.imgur.com'}\n",
            "{'/r/Math'}\n",
            "{'/r/learnmath'}\n",
            "{'/r/HomeworkHelp'}\n",
            "{'/r/askscience'}\n",
            "{'/r/AskPhysics'}\n",
            "{'/r/AskStatistics'}\n",
            "{'/r/math'}\n",
            "{'/r/statistics'}\n",
            "{'/r/shittymath'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OR try this implementation that just gets the HREF's from the elements that it finds and gets them in string form.\n",
        "def parse_href(url, element):\n",
        "    session = HTMLSession()\n",
        "    r = session.get(url)\n",
        "    elements = r.html.find(element)\n",
        "    links = [element.attrs['href'] for element in elements if 'href' in element.attrs]\n",
        "    return links\n"
      ],
      "metadata": {
        "id": "7Lrnc4Oi8T7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_url = 'https://www.reddit.com/r/askmath/'\n",
        "hyperlinks = parse_href(reddit_url, 'a')\n",
        "\n",
        "# Print the string versions of the hyperlinks\n",
        "for hyperlink in hyperlinks:\n",
        "    print(hyperlink)\n"
      ],
      "metadata": {
        "id": "TpNYWDX2DAbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*we may not get here today*\n",
        "There are some interesting things we can do with NLTK when analyzing text data. Such as breaking a paragraph into its constituent sentences."
      ],
      "metadata": {
        "id": "QReT75a18UT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#takes a paragraph as string and breaks it down into sentences\n",
        "def sentence_list(paragraph_string):\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    list_of_sentences = tokenizer.tokenize(paragraph_string)\n",
        "    return list_of_sentences"
      ],
      "metadata": {
        "id": "9VytK-2o8HNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we apply a rules based themes/sentiment analysis like we saw yesterday to the data we scrape from the web?\n",
        "Yes."
      ],
      "metadata": {
        "id": "TpbDiW6XEplI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1BnWrKqEohC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}