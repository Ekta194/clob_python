{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ekta194/clob_python/blob/main/Day_5_HTML_requests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qy5x2r3w5a3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Make the HTTP request\n",
        "url = 'https://medium.com/towards-data-science/can-chatgpt-write-better-sql-than-a-data-analyst-f079518efab2'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content with Beautiful Soup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Remove unwanted elements (script, style, comments, etc.)\n",
        "for element in soup(['script', 'style', 'comment']):\n",
        "    element.extract()\n",
        "\n",
        "# Extract the visible text\n",
        "text = soup.get_text(separator=' ')\n",
        "\n",
        "# Print the extracted text\n",
        "print(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_and_parse(url):\n",
        "  response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content with Beautiful Soup\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Remove unwanted elements (script, style, comments, etc.)\n",
        "  for element in soup(['script', 'style', 'comment']):\n",
        "      element.extract()\n",
        "\n",
        "# Extract the visible text\n",
        "  text = soup.get_text(separator=' ')\n",
        "  return text\n",
        "\n"
      ],
      "metadata": {
        "id": "FFt4JOsI7--0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 requests-html\n"
      ],
      "metadata": {
        "id": "zCaHPJky63_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b88c8b5-1931-46ce-f44c-d92e4974c860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting parse (from requests-html)\n",
            "  Downloading parse-1.19.1-py2.py3-none-any.whl (18 kB)\n",
            "Collecting bs4 (from requests-html)\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (1.4.4)\n",
            "Collecting importlib-metadata>=1.4 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.65.0)\n",
            "Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests-html) (4.9.2)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.15.0)\n",
            "Building wheels for collected packages: bs4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=1ecac6426b446d311f10f4176e8993d3084037911a14307525d553d9a1551893\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "Successfully built bs4\n",
            "Installing collected packages: pyee, parse, fake-useragent, websockets, w3lib, importlib-metadata, cssselect, pyquery, pyppeteer, bs4, requests-html\n",
            "Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 importlib-metadata-6.8.0 parse-1.19.1 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests-html-0.10.0 w3lib-2.1.1 websockets-10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import numpy as np\n",
        "import nltk.data\n",
        "from requests_html import HTMLSession"
      ],
      "metadata": {
        "id": "mIpPEu8v8AA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_elements(url,element):\n",
        "    session = HTMLSession()\n",
        "    r = session.get(url)\n",
        "    elements = r.html.find(element)\n",
        "    return elements"
      ],
      "metadata": {
        "id": "d00LzkUZ8BFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://medium.com/towards-data-science/ai-scaling-why-keeping-up-is-essential-and-how-to-do-it-56be3c2e1e5'"
      ],
      "metadata": {
        "id": "mR5_9sTA8GSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = parse_elements(url, 'p')\n",
        "links = parse_elements(url, 'a')"
      ],
      "metadata": {
        "id": "xYmg7zid8HDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_detection(sentence):\n",
        "    positive_words = ('happy', 'sunny', 'positive', 'triumphant', 'optimistic', 'wonderful')\n",
        "    negative_words = ('sad', 'terrible', 'frightening', 'rainy', 'scary', 'shocked')\n",
        "\n",
        "    positive = any(sentence.count(i) > 0 for i in positive_words)\n",
        "    negative = any(sentence.count(i) > 0 for i in negative_words)\n",
        "\n",
        "    if positive == negative == False:\n",
        "        return \"neutral\"\n",
        "    elif positive != negative:\n",
        "        return \"positive\" if positive else \"negative\"\n",
        "    else:\n",
        "        return \"mixed\""
      ],
      "metadata": {
        "id": "bYNYeberVUUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGLV9IUtWeOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#requirements print out a sentiment analysis after every paragraph prints out\n",
        "#print(paragraphs)\n",
        "for p in range(0, len(paragraphs)):\n",
        "    print(paragraphs[p].text)"
      ],
      "metadata": {
        "id": "_NJZgGMVRmv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_url = 'https://www.reddit.com/r/askmath/'\n",
        "hyperlinks = parse_elements(reddit_url, 'a')"
      ],
      "metadata": {
        "id": "S0apadGL8T1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the string versions of the hyperlinks\n",
        "for hyperlink in hyperlinks:\n",
        "    print(hyperlink)\n"
      ],
      "metadata": {
        "id": "6CLRABsC8T4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OR try this implementation that just gets the HREF's from the elements that it finds and gets them in string form.\n",
        "def parse_href(url, element):\n",
        "    session = HTMLSession()\n",
        "    r = session.get(url)\n",
        "    elements = r.html.find(element)\n",
        "    links = [element.attrs['href'] for element in elements if 'href' in element.attrs]\n",
        "    return links\n"
      ],
      "metadata": {
        "id": "7Lrnc4Oi8T7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_url = 'https://www.reddit.com/r/askmath/'\n",
        "hyperlinks = parse_href(reddit_url, 'a')\n",
        "\n",
        "# Print the string versions of the hyperlinks\n",
        "for hyperlink in hyperlinks:\n",
        "    print(hyperlink)\n"
      ],
      "metadata": {
        "id": "TpNYWDX2DAbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*we may not get here today*\n",
        "There are some interesting things we can do with NLTK when analyzing text data. Such as breaking a paragraph into its constituent sentences."
      ],
      "metadata": {
        "id": "QReT75a18UT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#takes a paragraph as string and breaks it down into sentences\n",
        "def sentence_list(paragraph_string):\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    list_of_sentences = tokenizer.tokenize(paragraph_string)\n",
        "    return list_of_sentences"
      ],
      "metadata": {
        "id": "9VytK-2o8HNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we apply a rules based themes/sentiment analysis like we saw yesterday to the data we scrape from the web?\n",
        "Yes."
      ],
      "metadata": {
        "id": "TpbDiW6XEplI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1BnWrKqEohC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}